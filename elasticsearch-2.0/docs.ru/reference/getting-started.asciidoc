[[getting-started]]
= Первые шаги

[partintro]
--

Elasticsearch - это высокомасштабируемый движок для полнотекстового поиска и аналитики, поставляемый с открытыми исходными кодами. Он позволяет хранить, искать и анализировать большие объёмы данных быстро и почти в реальном времени. Обычно он используется как базовый движок/технология, для приложений, которые имеют сложные особенности поиска и требования к поиску.

Здесь перечислены некоторые случаи, в которых может быть использован Elasticsearch:

* Вы запускаете онлайновый веб-магазин, где вы разрешаете вашим клиентам искать продукты, которые вы продаёте. В этом случае, вы можете использовать Elasticsearch для хранения вашего полного каталога товаров, а также инвентаризировать его и предоставлять поиск и автозаполнение подсказок для них.
* Вы хотите хранить лог или данные транзакций и анализировать и извлекать эти данные для просмотра тенденций, статистики, итогов или аномалий. В этом случае вы можете использовать Logstash (часть стека Elasticsearch/Logstash/Kibana) для сбора, агрегации и разбора ваших данных, а затем создать Logstash-ленту этих данных в Elasticsearch. После того как однажды вы поместили эти данные в Elasticsearch, вы сможете запускать поисковые запросы и агрегации для извлечения любой информации, которая вам интересна.
* Вы запускаете платформу оповещения о ценах, которая позволяет подкованным в ценах клиентам задать правило типа "Мне интересна продажа конкретного электронного гаджета и я хочу получить извещение, если цена этого гаджета упадёт в течение следующего месяца ниже $X у любого из поставщиков". В этом случае вы можете собрать цены поставщиков, разместить их в Elasticsearch и использовать его возможность обратного поиска (Percolator) для того, чтобы найти изменения цен согласно запросам клиентов, а также сгенерировать уведомление клиенту при нахождении совпадений.
*  У вас есть потребности в аналитике/бизнес-аналитике и вы хотите быстро исследовать, проанализировать, визуализировать и задать нерегламентированные вопросы для большого объёма данных (миллионы или миллиарды записей). В этом случае вы можете использовать Elasticsearch для хранения ваших данных и затем использовать Kibana (часть стека Elasticsearch/Logstash/Kibana) для построения собственных панелей инструментов, которые могут визуализировать аспекты ваших данных, являющиеся важными для вас. В дополнение, вы можете использовать функционал агрегирования Elasticsearch для выполнения сложных запросов бизнес-аналитики на ваших данных.

В оставшейся части этого учебника я проведу вас через процесс получения и запуска Elasticsearch, мы заглянем внутрь и научимся выполнять базовые операции, такие как индексирование, поиск и изменение ваших данных. В конце этого учебника у вас должно появиться понимание того, что такое Elasticsearch, как он работает, и надеемся, что вы увидите, как его можно использовать для построения сложных поисковых приложений или для извлечения аналитики из ваших данных.
--

[[_basic_concepts]]
== Базовые понятия

Ниже приведены несколько понятий, которые являются ключевыми для Elasticsearch. Их понимание с самого начала, сильно поможет упростить процесс обучения.

[float]
=== Почти реальное время (NRT)

ElasticSearch является NRT поисковой платформой. Что означает очень малое время отклика (обычно одна секунда) от момента, когда вы проиндексировали документ, до момента, когда он становится доступным к поиску.

[float]
=== Кластер (cluster)

Кластер - это коллекция из одного или более серверов, называемых "узел" (node), которые вместе хранят весь объём данных и предоставляют возможности индексирования и поиска прозрачно с использованием всех узлов. Кластер идентифицируется уникальным именем по умолчанию "elasticsearch". Имя важно, посколько узел может быть частью кластера, только если эта нода настроена на присоединение к кластеру по его имени.

Убедитесь, что вы не используете одинаковые имена кластера в разных окружениях, в противном
случае, вы можете в конечном итоге подключить ноды не к тому кластеру. Например, вы можете
использовать `loggin-dev`, `loggin-stage` и `loggin-prod` для кластеров, которые используются
соответственно для целей разработки, постановки и продуктива.

Обратите внимание, что это правильно и даже превосходно иметь кластер с только одним узлом в нём. Однако, вы можете также иметь несколько независимых кластеров, каждый из которых будет иметь уникальное имя.

[float]
=== Узел (node)

Узел - это одиночный сервер, который является частью вашего кластера и предоставляет возможности индексирования и поиска. Как и кластер, узел идентифицируется именем, которое по умолчанию выбирается по случайному алгоритму Marvel для символьных имён и присваивается узлу при его запуске. Вы можете задать любое имя узла, которое вы хотите, если вас не устраивает умолчание. Имя важно для административных целей, чтобы идентифицировать какой сервер в сети является каким узлом кластера Elasticsearch.

Узел можно сконфигурировать на присоединение к заданному кластеру по имени кластера. По умолчанию, каждый узел присоединяется к кластеру с именем `elasticsearch`, т.е. если вы запустите несколько узлов в вашей сети с настройками по умолчанию, они автоматически сформируют один кластер с именем `elasticsearch`.

Кластер может включать произвольное количество узлов. Однако, если у вас нет других узлов Elasticsearch в этой же сети, запуск одного узла по умолчанию создаст кластер с именем `elasticsearch` из одного узла, куда и будет присоединён этот узел.

[sect2]
[float]
=== Индекс (index)

Индекс - это коллекция документов, которые имеют некие похожие характеристики. Например, у вас может быть один индекс с данными о клиентах, другой индекс с каталогом товаров, а также ещё индекс с данными заказов. Индекс идентифицируется по имени (которое должно быть в нижнем регистре) и это имя используется как ссылка на этот индекс при выполнении операций индексирования, поиска, обновления и удаления документов, содержащихся в индексе.

В одном кластере, вы можете создать произвольное количество индексов.

[float]
=== Тип (type)

Внутри индекса, вы можете определить один или несколько типов. Тип - это логическая категория/часть вашего индекса, смысл которого полностью зависит от вас. Обычно, тип определяется для документов, которые имеют много общих полей. Например, пусть у вас есть блог и все ваши данные хранятся в одном индексе. В этом индексе вы можете определить один тип для данных пользователя, другой тип для данных блога и ещё тип для комментариев.

[float]
=== Документ (document)

Это базовый элемент информации, которая может быть проиндексирована. Например, у вас может быть один документ для одного клиента, другой документ для одного товара и ещё для одного заказа. Документ составляется в виде выражения http://json.org/[JSON] (JavaScript Object Notation), который повсеместно используется в Интернет для обмена информацией.

Вместе c каким-либо индексом/типом, вы можете хранить произвольное количество документов. Обратите внимание, что хотя документ физически находится в индексе, документ фактически должен быть индексирован/назначен для какого-либо типа внутри индекса.

TIP: Если вы знакомы с SQL базами данных, то, возможно, вам будет проще понять что такое индекс, тип и документ, если сказать, что индекс - это аналог базы данных, тип - аналог таблицы, а документ - аналог записи в таблице (строки). (прим. переводчика)

[float]
=== Шарды (shards) и Реплики (replicas)

Индекс потенциально может хранить большое количество данных, которое может превышать аппаратные ограничения на одном узле. Например, один индекс одного миллиарда документов может занимать свыше 1TB места на диске, что может не поместится на диск одного узла или может быть слишком медленным для обслуживания поисковых запросов на одном узле.

Чтобы решить эту проблему, Elasticsearch предоставляет возможность разделения вашего индекса на несколько частей, которые называются шардами. Когда вы создаёте индекс, вы можете просто задать необходимое вам количество шардов. Каждый шард является самодостаточным и полнофункциональным независимым "индексом", который может размещаться на любом узле кластера.

Шардинг является важным по двум основным причинам:

* Он позволяет вам горизонтальное разбиение/масштабирование тома ваших данных
* Он позволяет вам распределять и выполнять параллельно операции с шардами (потенциально на нескольких узлах) что увеличивает производительность/пропускную способность

Механика распределения операций по шардам а также обратная агрегация находящихся в них документов в результаты поиска, полностью управляется Elasticsearch и прозрачна для пользователя.

В сетевом/облачном окружении, где отказ может произойти в любое время, очень полезно и очень рекомендуется иметь механизм отказоустойчивости в случае, когда какой-либо шард/узел становится недоступным по какой-либо причине. В заключение, Elasticsearch позволяет вам сделать одну или более копий шардов вашего индекса, которые называются репликами шардов или для краткости просто репликами.

Репликация важна по двум основным причинам:

* Она предоставляет высокую доступность в случае отказа какого-либо шарда/узла. По этой причине, важно, чтобы реплика шарда никогда не размещалась на том же самом узле, где находится оригинальный/первичный шард, с которого делалась копия.
* Она позволяет вам масштабировать пропускную способность вашего поиска, поскольку поиск может быть запущен на всех репликах параллельно.

В качестве итога, каждый индекс может быть разбит на несколько шардов. Индекс может также быть реплицирован ноль (означает не реплицирован) или более раз. Однажды реплицированный, каждый индекс будет иметь первичные шарды (оригинальные шарды, с которых осуществлялась репликация) и реплики шардов (копии первичных шардов).
Количество шардов и реплик может быть задано для индекса в момент его создания. После того как индекс создан, вы можете изменить количество реплик динамически в любое время, но количество шардов постфактум изменить не можете.

По умолчанию, под каждый индекс в Elasticsearch выделятся 5 первичных шардов и 1 реплика, что означает, что если у вас как минимум два узла в кластере, ваш индекс имеет 5 первичных шардов и другие 5 реплик шардов (1 полная реплика) т.е. всего 10 шардов на индекс.

NOTE: Каждый шард в Elasticsearch является индексом Lucene. Существует ограничение на максимальное количество документов, которые вы можете разместить в одном индексе Lucene. Согласно https://issues.apache.org/jira/browse/LUCENE-5843[`LUCENE-5843`], это ограничение составляет 2,147,483,519 (= Integer.MAX_VALUE - 128) документов. Вы можете мониторить размеры шардов, используя <<cat-shards,`_cat/shards`>> api.

Теперь давайте начнём, с чего-либо интересного...

[[_installation]]
== Установка

Elasticsearch требует Java 7. В частности, рекомендуется, чтобы вы использовали Oracle JDK {jdk}. Установка Java различается от платформы к платформе, так что мы не можем описать её здесь подробно. Рекомендованную Oracle документацию по установке можно найти http://docs.oracle.com/javase/8/docs/technotes/guides/install/install_overview.html[на сайте Oracle]. Перед установкой Elasticsearch, пожалуйста, сперва проверьте вашу версию Java (и затем установите/обновите, если нужно):

[source,sh]
--------------------------------------------------
java -version
echo $JAVA_HOME
--------------------------------------------------

После того как вы установили Java, вы можете загрузить и запустить Elasticsearch. Последние релизы Elasticsearch в бинарном виде доступны на странице http://www.elastic.co/downloads[`www.elastic.co/downloads`], включая все версии, выпущенные в прошлом. Для каждого релиза вы можете выбрать формат архива `zip`, `tar`, `DEB` или `RPM` пакет. Для простоты, пусть мы будем использовать tar архив.

Скачайте Elasticsearch {version} в tar формате следующим образом (Пользователи Windows должны, по идее, скачивать zip архив):

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
curl -L -O https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-{version}.tar.gz
--------------------------------------------------

Затем распакуйте архив (пользователи Windows должны использовать для zip архива unzip):

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
tar -xvf elasticsearch-{version}.tar.gz
--------------------------------------------------

Команда создаст файлы и подкаталоги в вашем текущем каталоге. Затем перейдите в подкаталог bin:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
cd elasticsearch-{version}/bin
--------------------------------------------------

И теперь мы готовы запустить наш узел и кластер (пользователи Windows должны запустить файл elasticsearch.bat):

[source,sh]
--------------------------------------------------
./elasticsearch
--------------------------------------------------

Если всё пошло хорошо, мы должны увидеть пачку сообщений, которые выглядят примерно так:

["source","sh",subs="attributes,callouts"]
--------------------------------------------------
./elasticsearch
[2014-03-13 13:42:17,218][INFO ][node           ] [New Goblin] version[{version}], pid[2085], build[5c03844/2014-02-25T15:52:53Z]
[2014-03-13 13:42:17,219][INFO ][node           ] [New Goblin] initializing ...
[2014-03-13 13:42:17,223][INFO ][plugins        ] [New Goblin] loaded [], sites []
[2014-03-13 13:42:19,831][INFO ][node           ] [New Goblin] initialized
[2014-03-13 13:42:19,832][INFO ][node           ] [New Goblin] starting ...
[2014-03-13 13:42:19,958][INFO ][transport      ] [New Goblin] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.8.112:9300]}
[2014-03-13 13:42:23,030][INFO ][cluster.service] [New Goblin] new_master [New Goblin][rWMtGj3dQouz2r6ZFL9v4g][mwubuntu1][inet[/192.168.8.112:9300]], reason: zen-disco-join (elected_as_master)
[2014-03-13 13:42:23,100][INFO ][discovery      ] [New Goblin] elasticsearch/rWMtGj3dQouz2r6ZFL9v4g
[2014-03-13 13:42:23,125][INFO ][http           ] [New Goblin] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.8.112:9200]}
[2014-03-13 13:42:23,629][INFO ][gateway        ] [New Goblin] recovered [1] indices into cluster_state
[2014-03-13 13:42:23,630][INFO ][node           ] [New Goblin] started
--------------------------------------------------

Не вдаваясь в подробности, мы можем увидеть, что наш узел с именем "New Goblin" (который будет в нашем случае другим персонажем Marvel) запустился и выбрал себя как master в одиночном кластере. Не беспокойтесь пока о том, что означает "master". Основная вещь, которая сейчас важна - это то, что мы запустили один узел внутри одного кластера.

Как говорилось ранее, мы можем перекрыть либо имя кластера, либо имя узла. Это можно выполнить из командной строки, при запуске Elasticsearch:

[source,sh]
--------------------------------------------------
./elasticsearch --cluster.name my_cluster_name --node.name my_node_name
--------------------------------------------------

Также обратите внимание на строку, где есть слово http с информацией об адресе HTTP (`192.168.8.112`) и порте (`9200`), которые будет использовать наш узел. По умолчанию, Elasticsearch использует для предоставления доступа к своему внутреннему REST API порт `9200`. Данный порт можно настроить, если это необходимо.

[[_exploring_your_cluster]]
== Исследование вашего кластера

[float]
=== REST API

Теперь, когда вы запустили узел (и кластер), следующий шаг состоит в понимании того как вы будете взаимодействовать с ним. К счастью, Elasticsearch предоставляет очень понятный и мощный REST API, который вы можете использовать для работы с вашим кластером. Среди прочих вещей, которые можно выполнять с помощью данного API есть следующие:

* Проверять работоспособность вашего кластера, узела и индекса, смотреть их состояние и статистику
* Управлять вашим кластером, узлом и данными индекса и метаданными
* Выполнять операции CRUD (Create, Read, Update, and Delete) - (Создание, Чтение, Обновление и Удаление) и операции поиска для ваших индкесов
* Выполнять расширенные поисковые операции, такие как пейджинг (постраничный результат), сортировка, фильтрация, скриптинг, агрегация и многое другое.


[[_cluster_health]]
=== Работоспобность кластера

Начнём с базовой проверки работоспособности, которую мы можем использовать для того, чтобы увидеть, что делает кластер. Мы будем использовать curl, чтобы сделать это, но вы можете использовать любой инструмент, который позволит вам выполнять HTTP/REST запросы. Будем считать, что мы находимся на том же узле, где мы запустили Elasticsearch и откроем новое окно с командной строкой.

Чтобы проверить работоспособность кластера, мы будет использовать <<cat,`_cat` API>>. Сперва запомните, что точка доступа к наш узлу через HTTP находится на порту `9200`:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/_cat/health?v'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
epoch      timestamp cluster       status node.total node.data shards pri relo init unassign
1394735289 14:28:09  elasticsearch green           1         1      0   0    0    0        0
--------------------------------------------------

Вы можете увидеть, что наш кластер с именем "elasticsearch" работает и имеет статус green.

Когда бы мы не проверили работоспособность кластера, мы получим либо green либо yellow либо red. Green (зелёный) означает что всё хорошо (кластер полностью работоспособен), yellow (жёлтый) означает, что все данные доступны, но некоторые реплики ещё не размещены (кластер полностью функционален) и red (красный) означает, что некоторые данные недоступны по какой-либо причине. Обратите внимание, что даже если кластер находится в состоянии red, он продолжает частично работать (т.е. будет продолжать обслуживать поисковые запросы для доступных шардов), но вам будет необходимо починить их как можно скорее, поскольку у вас есть потерянные данные.

Также из вышеприведённого ответа, мы можем увидеть общее количество узлов в кластере (total) 1 и что у нас 0 шардов (поле shards), поскольку у нас ещё нет никаких данных. Обратите внимание, что мы используем имя кластера по умолчанию (elasticsearch) и так как Elasticsearch использует обнаружение других узлов в сети через unicast запросы, то вполне возможно, что вы могли случайно запустить более чем один узел в вашей сети и они все присоедились к одному кластеру. В этом случае, вы можете увидеть в вышеприведённом ответе более чем 1 узел.

Мы можем также получить список узлов кластера с помощью запроса:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/_cat/nodes?v'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/_cat/nodes?v'
host         ip        heap.percent ram.percent load node.role master name
mwubuntu1    127.0.1.1            8           4 0.00 d         *      New Goblin
--------------------------------------------------

Здесь мы можем увидеть один наш узел с именем "New Goblin", который является единственным узлом, который в настоящий момент находится в вашем кластере.

[[_list_all_indices]]
=== Список всех индексов

Теперь мы получим список наших индексов:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/_cat/indices?v'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/_cat/indices?v'
health index pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------

Означает, просто что у нас пока нет индексов в кластере.

[[_create_an_index]]
=== Создание индекса

Теперь мы создадим индекс с именем "customer" и затем снова выведем список всех индексов:

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer?pretty'
curl 'localhost:9200/_cat/indices?v'
--------------------------------------------------

Первая команда создаёт индекс с именем "customer", используя действие PUT. Мы просто добавляем `pretty` к концу вызова, чтобы сказать о том, что JSON ответ нужно выдать в красивом виде (если надо).

ответ:

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer?pretty'
{
  "acknowledged" : true
}

curl 'localhost:9200/_cat/indices?v'
health index    pri rep docs.count docs.deleted store.size pri.store.size
yellow customer   5   1          0            0       495b           495b
--------------------------------------------------

Результаты второй команды говорят нам, что у нас теперь есть 1 индекс с именем customer и он имеет 5 первичных шардов и 1 репику (по умолчанию) а также, что он содержит 0 документов.

Вы также можете обратить внимание, что индекс customer находится в состоянии yellow. Возвращаясь к написанному ранее, это означает, что некоторые реплики (ещё) не размещены. Причина заключается в том, что для этого индекса, Elasticsearch по умолчанию создал одну реплику на индекс. Поскольку у нас, в данный момент, запущен только один узел, то эта реплика не может быть пока размещена (для высокой доступности) до тех пор, пока в дальнейшем к кластеру не присоединится ещё один узел. Как только эта реплика разместится на втором узле, статус для этого индекса станет зелёным.


[[_index_and_query_a_document]]
=== Индексирование и запрос документа

Теперь мы что-либо поместим в наш индекс customer. Сперва запомните, что для индексирования документа, вы должны сказать Elasticsearch, какой тип должен быть в индексе.

Пусть индексируется простой customer документ, помещаемый в индекс customer, имеющий тип "external", с ID имеющим значение 1:

Наш JSON документ: { "name": "John Doe" }

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '
{
  "name": "John Doe"
}'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '
{
  "name": "John Doe"
}'
{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "1",
  "_version" : 1,
  "created" : true
}
--------------------------------------------------

Из вышеприведённого, мы можем увидеть, что новый документ был успешно создан внутри индекса customer с типом external. Документ также имеет внутренний id со значением 1, которое мы указали в момент индексирования.

Важным замечением является то, что Elasticsearch не требует от вас сперва явно создать индекс перед тем как вы будете индексировать документ в этот индекс. В предыдущем примере, Elasticsearch будет автоматически создавать индекс customer, если он уже не существовал до этого.

Теперь получим документ, который мы только что проиндексировали:

[source,sh]
--------------------------------------------------
curl -XGET 'localhost:9200/customer/external/1?pretty'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
curl -XGET 'localhost:9200/customer/external/1?pretty'
{
  "_index" : "customer",
  "_type" : "external",
  "_id" : "1",
  "_version" : 1,
  "found" : true, "_source" : { "name": "John Doe" }
}
--------------------------------------------------

Ничего неординарного тут нет, кроме поля `found`, в котором говорится, что найден документ с запрошенным ID 1 и другое поле, `_source`, которое возвращает полный документ в формате JSON, который мы проиндексировали на предыдущем шаге.

[[_delete_an_index]]
=== Удаление индекса

Теперь удалим индекс, который мы только что создали и снова получим список всех индексов:

[source,sh]
--------------------------------------------------
curl -XDELETE 'localhost:9200/customer?pretty'
curl 'localhost:9200/_cat/indices?v'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
curl -XDELETE 'localhost:9200/customer?pretty'
{
  "acknowledged" : true
}
curl 'localhost:9200/_cat/indices?v'
health index pri rep docs.count docs.deleted store.size pri.store.size
--------------------------------------------------

Что означает, что индекс успешно удалён и мы снова вернулись к началу, когда у нас не было ни одного индекса в кластере.

Перед тем как двигаться дальше, мы расскажем об API ещё нескольких команд:

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer'
curl -XPUT 'localhost:9200/customer/external/1' -d '
{
  "name": "John Doe"
}'
curl 'localhost:9200/customer/external/1'
curl -XDELETE 'localhost:9200/customer'
--------------------------------------------------

Если внимательно изучить вышеописанные команды, мы можем фактически увидеть образец того как получать доступ к данным в Elasticsearch. Этот образец в итоге выглядит так:

[source,sh]
--------------------------------------------------
curl -X<REST Verb> <Node>:<Port>/<Index>/<Type>/<ID>
--------------------------------------------------

Образец REST доступа распространяется на все команды API, так что если вы просто запомните его, это будет хорошим стартом в освоении Elasticsearch.


[[_modifying_your_data]]
== Изменение ваших данных

Elasticsearch предоставляет манипуляцию с данными и возможности поиска с временем, которое близко к реальному. По умолчанию, вы можете ожидать секундной задержки (интервал обновления) с момента, когда вы начали индексирование/обновление/удаление ваших данных до момента, когда всё это появится в результатах поиска. Это важное ограничение по сравнению с другими платформами, такими как SQL, где данные становятся немедленно доступными после завершения транзакции.

[float]
=== Индексирование/замена документов

Ранее мы увидели как можно проиндексировать одиночный документ. Давайте вызовем эту команду снова:

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '
{
  "name": "John Doe"
}'
--------------------------------------------------

Опять эта команда будет индксировать заданный документ в индекс customer, с типом external и ID равным 1. Если вы затем запустим эту команду снова с другим (или этим же) документом, Elasticsearch заменит (т.е. переиндексирует) новым документом существующий документ с ID равным 1:

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer/external/1?pretty' -d '
{
  "name": "Jane Doe"
}'
--------------------------------------------------

Выше показано изменение имени документа с ID равным 1 с "John Doe" на "Jane Doe". Если, с другой стороны, мы используем другой ID, будет проиндексирован новый документ, а тот что уже находится в индексе, оставлен без изменений.

[source,sh]
--------------------------------------------------
curl -XPUT 'localhost:9200/customer/external/2?pretty' -d '
{
  "name": "Jane Doe"
}'
--------------------------------------------------

Данная выше команда индексирует новый документ с ID равным 2.

При индексировании, ID часть является необязательной. Если она не задана, Elasticsearch будет генерировать случайный ID и затем использовать его для индексирования документа. Фактический ID, который генерирует Elasticsearch (или который мы указали явно в предыдущем примере), возвращается как часть вызова API индексирования.

Данный пример показывает как индексировать документ без явного указания ID:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/customer/external?pretty' -d '
{
  "name": "Jane Doe"
}'
--------------------------------------------------

Обратите внимание, что в вышеуказанном случае, мы используем POST вместо PUT, так как мы не указали ID.


[[_updating_documents]]
=== Обновление документов

В дополнение к индексированию и замене документов, мы также можем обновлять документы. Обратите внимание, что Elasticsearch фактически не может реализовать обновления как таковые. Всякий раз, когда мы делаем обновление, Elasticsearch удаляет старый документ и затем индексирует новый документ за один шаг.

Данный пример показывает как обновить наш предыдущий документ (с ID равным 1), изменив в нём поле имени на "Jane Doe":

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '
{
  "doc": { "name": "Jane Doe" }
}'
--------------------------------------------------

Данный пример показывает как обновить наш предыдущий документ (с ID равным 1), изменив поле именим на "Jane Doe" и в это же время добавить к документу поле возраста:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '
{
  "doc": { "name": "Jane Doe", "age": 20 }
}'
--------------------------------------------------

Обновления также могут быть выполнены с помощью простых скриптов. Обратите внимание, что динамические скрипты по умолчанию выключены с версии `1.4.3`, см. подробности в <<modules-scripting,scripting docs>>. Данный пример использует скрипт для увеличения возраста на 5:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/customer/external/1/_update?pretty' -d '
{
  "script" : "ctx._source.age += 5"
}'
--------------------------------------------------

В вышеприведённом примере, `ctx._source` указывает на исходный текущий документ, который мы обновили выше.

Обратите внимание, что обновления могут быть выполнены в один момент времени только для одного документа. В будущем, Elasticsearch будет предоставлять возможность обновить несколько документов, предоставляя условия запроса (как в SQL операторе `UPDATE-WHERE`).

[[_deleting_documents]]
=== Удаление документов

Удаление документа столь же просто. Данный пример показывает как удалить наш предыдущий документ с ID равным 2:

[source,sh]
--------------------------------------------------
curl -XDELETE 'localhost:9200/customer/external/2?pretty'
--------------------------------------------------

Плагин `delete-by-query` может удалить все документы, совпадающие с определённым запросом.

[[_batch_processing]]
=== Пакетная обработка

В дополнение к индексированию, обновлению и удалению отдельных документов, Elasticsearch также предоставляет возможность выполнять любые вышеописанные операции в пакетом (массовом) режиме, используя <<docs-bulk,`_bulk` API>>. Данная функциональность важна тем, что она предоставляет очень эффективный механизм для выполнения множественных операций так быстро, как это только возможно, с минимально возможной пересылкой данных по сети в обе стороны.

В качестве быстрого примера, следующий вызов индексирует два документа (ID 1 - John Doe и ID 2 - Jane Doe) одной массовой операцией:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '
{"index":{"_id":"1"}}
{"name": "John Doe" }
{"index":{"_id":"2"}}
{"name": "Jane Doe" }
'
--------------------------------------------------

Данный пример обновляет первый документ (с ID равным 1) и затем удаляет второй документ (с ID равным 2) в одной массовой операции:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/customer/external/_bulk?pretty' -d '
{"update":{"_id":"1"}}
{"doc": { "name": "John Doe becomes Jane Doe" } }
{"delete":{"_id":"2"}}
'
--------------------------------------------------

Обратите внимание, что для действия удаления не указан соответствующий исходный документ, потому что удаление требует только ID удаляемого документа.

API массовых операций выполняет все действия последовательно и по порядку. Если одно действие завершилось неудачей по какой-либо причине, оставшиеся действия после него продолжат выполняться. Когда происходит возврат из API массовой операции, будет предоставлен статус каждой операции (в том же порядке, в каком шли операции), так что вы можете проверить удачным было указанное действие или нет.

[[_exploring_your_data]]
== Исследование ваших данных

[float]
=== Простой набор данных

Теперь, когда мы ознакомились с основами, попытаемся поработать с более реалистичным набором данных. Я подготовил набор JSON документов с инофрмацией о клиентах банка. Каждый документ имеет следующую схему:

[source,sh]
--------------------------------------------------
{
    "account_number": 0,
    "balance": 16623,
    "firstname": "Bradshaw",
    "lastname": "Mckenzie",
    "age": 29,
    "gender": "F",
    "address": "244 Columbus Place",
    "employer": "Euron",
    "email": "bradshawmckenzie@euron.com",
    "city": "Hobucken",
    "state": "CO"
}
--------------------------------------------------

Для любопытствующих, я сгенерировал эти данных на http://www.json-generator.com/[`www.json-generator.com/`] так что, пожалуйста, игнорируйте значения и смысл данных, так как все они случайно сгенерированы.

[float]
=== Загрузка набора данных

Вы можете загрузить этот набор данных (accounts.json) https://github.com/bly2k/files/blob/master/accounts.zip?raw=true[отсюда]. Извлеките содержимое архива в текущий каталог и затем загрузите его в кластер:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/account/_bulk?pretty' --data-binary "@accounts.json"
curl 'localhost:9200/_cat/indices?v'
--------------------------------------------------

ответ:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/_cat/indices?v'
health index pri rep docs.count docs.deleted store.size pri.store.size
yellow bank    5   1       1000            0    424.4kb        424.4kb
--------------------------------------------------

означает, что мы успешно выполнили массовое индексирование 1000 документов в индекс с именем bank (и типом account).

[[_the_search_api]]
=== API поиска

Теперь выполним несколько простых поисков. Есть два базовых способа запустить поиск: один состоит в отправке параметров поиска через <<search-uri-request,URI REST запроса>>, а другой через отправку этих же параметров через <<search-request-body,тело REST запроса>>. Метод запроса через тело, позволяет вам большую выразительность, а также определяет параметры поиска в более читабельном для человека JSON формате. Мы попытаемся выполнить один пример с помощью метода URI, но далее мы будем использовать исключительно метод с телом запроса.

REST API поиска доступно через указание `_search`. Данный пример, вернёт все документы в индексе bank:

[source,sh]
--------------------------------------------------
curl 'localhost:9200/bank/_search?q=*&pretty'
--------------------------------------------------

Давайте сперва проанализируем вызов поиска. Мы ищем (указываем `_search`) в индексе bank и параметр `q=*` инструктирует Elasticsearch искать совпадение со всеми документами в индексе. Параметр `pretty`, снова, просто говорит Elasticsearch возвращать результаты в виде читаемого человеком JSON.

И ответ (показан частично):

[source,sh]
--------------------------------------------------
curl 'localhost:9200/bank/_search?q=*&pretty'
{
  "took" : 63,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "1",
      "_score" : 1.0, "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "6",
      "_score" : 1.0, "_source" : {"account_number":6,"balance":5686,"firstname":"Hattie","lastname":"Bond","age":36,"gender":"M","address":"671 Bristol Street","employer":"Netagy","email":"hattiebond@netagy.com","city":"Dante","state":"TN"}
    }, {
      "_index" : "bank",
      "_type" : "account",
--------------------------------------------------

В ответе, мы видим следующие части:

* `took` – время в миллисекундах, которое Elasticsearch выполнял запрос
* `timed_out` – говорит там, произошёл ли таймаут поиска или нет
* `_shards` – говорит сколько шардов участвовало в поиске, а также количество шардов поиск по которым был успешным/неуспешным
* `hits` – количество совпадений с результатами поиска
* `hits.total` – общее количетсво документов, совпавшее с критериями поиска
* `hits.hits` – фактический массив результатов поиска (по умолчанию первые 10 документов)
* `_score` и `max_score` - пока игнорируем эти поля

Здесь точно тот же поиск, что и выше, но с использованием альтернативного метода с телом запроса:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_all": {} }
}'
--------------------------------------------------

Разница здесь в том, что вместо указания `q=*` в URI, мы выполняем POST запрос с JSON в теле запроса для `_search` API. Мы расскажем о JSON запросах в следующем разделе.

И ответ (показан частично):

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_all": {} }
}'
{
  "took" : 26,
  "timed_out" : false,
  "_shards" : {
    "total" : 5,
    "successful" : 5,
    "failed" : 0
  },
  "hits" : {
    "total" : 1000,
    "max_score" : 1.0,
    "hits" : [ {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "1",
      "_score" : 1.0, "_source" : {"account_number":1,"balance":39225,"firstname":"Amber","lastname":"Duke","age":32,"gender":"M","address":"880 Holmes Lane","employer":"Pyrami","email":"amberduke@pyrami.com","city":"Brogan","state":"IL"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "6",
      "_score" : 1.0, "_source" : {"account_number":6,"balance":5686,"firstname":"Hattie","lastname":"Bond","age":36,"gender":"M","address":"671 Bristol Street","employer":"Netagy","email":"hattiebond@netagy.com","city":"Dante","state":"TN"}
    }, {
      "_index" : "bank",
      "_type" : "account",
      "_id" : "13",
--------------------------------------------------

Важно понимать, что как только вы получили результаты запроса, Elasticsearch полностью выполнил запрос и больше не поддерживает никакие виды ресурсов или открытые курсоры на сервере в ваших результатах. Это резко отличается от многих других платформ, таких как SQL, где вы можете изначально получить частичное подмножество результатов запроса, а затем вы должны постоянно возвращаться к серверу, если вы хотите, получить остаток или следующую страницу результатов, используя своего рода состояние курсора на стороне сервера.


[[_introducing_the_query_language]]
=== Введение в язык запросов

Elasticsearch предоставляет доменно-специфичный язык в стиле JSON, который вы можете использовать для выполнения запросов. Он называется <<query-dsl,Query DSL>>. Язык запросов является довольно всесторонним и может быть пугающим на первый взгляд, но лучший способ на самом деле узнать его - начать с нескольких базовых примеров.

Вернёмся назад к последнему нашему примеру, мы выполнили запрос:

[source,sh]
--------------------------------------------------
{
  "query": { "match_all": {} }
}
--------------------------------------------------

как показано выше, часть запроса `query` говорит нам об определении собственно запроса, а часть `match_all` - это просто тип запроса, который мы хотим выполнить. Запрос `match_all` просто ищет все документы в указанном индексе.

В дополнении к параметру `query`, мы также можем указать другие параметры, чтобы влиять на результаты поиска. Например, следующий запрос выполняет `match_all` и возвращает только первый документ:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_all": {} },
  "size": 1
}'
--------------------------------------------------

Обратите внимание, что если значение `size` не задано, по умолчанию оно составляет 10.

Данный пример выполняет `match_all` и возвращает документы с 11 по 20:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_all": {} },
  "from": 10,
  "size": 10
}'
--------------------------------------------------

Параметр `from` задаёт с какого документа (начиная с 0-го) в индексе нужно начинать, а параметр `size` как много документов будет возвращено, начиная с параметра `from`. Данная особенность полезна для реализации пейджинга (разбиения на страницы) результатов поиска. Обратите внимание, что если `from` не указан, то его значение по умолчанию 0.

Этот пример выполняет `match_all` и сортирует результаты по балансу учётной записи в обратном порядке и возвращает верхние 10 (по умолчанию) документов.

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_all": {} },
  "sort": { "balance": { "order": "desc" } }
}'
--------------------------------------------------

[[_executing_searches]]
=== Выполнение поиска

Теперь, когда мы познакомились с несколькими базовыми параметрами поиска, давайте копать глубже внутрь Query DSL. Давайте сперва посмотрим на возвращаемые поля документа. По умолчанию, как часть всех поисковых запросов, возвращается полный JSON документ. Это называется исходным документом (поле `_source` в совпавших результатах). Если мы не хотим, чтобы возвращался весь исходный документ, мы имеем возможность запросить, чтобы из исходного документа возвращались только некоторые поля.

Данный пример показывает как вернуть два поля, `account_number` и `balance` (вместо `_source`) при поиске:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_all": {} },
  "_source": ["account_number", "balance"]
}'
--------------------------------------------------

Обратите внимание, что вышеприведённый пример просто уменьшает количество полей в `_source`. Он будет продолжать возвращать одно поле с именем `_source`, но в него будут включены только поля `account_number` и `balance`.

Если говорить о каком-то аналоге из SQL, то вышеописанный пример чем-то похож на `SQL SELECT FROM` список_полей.

Теперь, давайте поработаем с частью `query`. Ранее, мы увидели как использовать `match_all` для поиска совпадение по всем документов. Теперь давайте познакомимся с новым запросом, который называется <<query-dsl-match-query,`match` запросом>> и который может осуществлять базовый поиск по полям (т.е. поиск выполняется по заданному полю или списку полей).

Данный пример возвращает учётную запись пользователя с номером учётной записи 20:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match": { "account_number": 20 } }
}'
--------------------------------------------------

Данный пример возвращает все учётные записи, которые содержат термин (видимо имеется в виду подстрока – прим. пер.) "mill" в поле address:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match": { "address": "mill" } }
}'
--------------------------------------------------

Данный пример возвращает все учётные записи, содержащие в поле address термин "mill" или термин "lane":

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match": { "address": "mill lane" } }
}'
--------------------------------------------------

Данный пример является вариантом `match` (`match_phrase`), который возвращает все учётные записи, содержащие в поле address фразу "mill lane":

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": { "match_phrase": { "address": "mill lane" } }
}'
--------------------------------------------------

Теперь давайте познакомимся с <<query-dsl-bool-query, `логическим` (`bool`(ean)) запросом>>. Запрос `bool` позволяет нам составлять более компактные запросы в более больших запросах, используя булеву логику.

Данный пример составляет два запроса `match` и возвращает все учётные записи, содержащие в поле address "mill" и "lane":

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": {
    "bool": {
      "must": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}'
--------------------------------------------------

В вышеприведённом примере, предложение `bool must` задаёт все запросы, которые должны иметь значение истина (true) для документа, который был выбран по критерию` match`.

Для контраста, данный пример составляет два запроса `match` и возвращает все учётные записи содержащие в поле address "mill" или "lane":

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": {
    "bool": {
      "should": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}'
--------------------------------------------------

В вышеприведённом примере, предложение `bool should` задаёт список запросов, любой из которых должен иметь значение истина (true) для документа, который был выбран по критерию `match`.

Данный пример составляет два `match` запроса и возвращает все учётные записи, которые не содержат в поле address ни "mill", ни "lane":

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": {
    "bool": {
      "must_not": [
        { "match": { "address": "mill" } },
        { "match": { "address": "lane" } }
      ]
    }
  }
}'
--------------------------------------------------

В вышеприведённом примере, предложение `bool must_not` задаёт список запросов ни один из которые не должен иметь значение истина (true) для документа, который был выбран по критерию `match`.

Мы можем одновременно комбинировать предложения `must`, `should` и `must_not` внутри запроса `bool`. Таким образом, мы можем составлять `bool` запросы внутри любого из этих `bool` предложений для составления сложных многоуровневых логических запросов.

Данный пример возвращает все учётные записи для всякого, кто старше 40 лет, но не проживает в штате Айдахо (аббревиатуа ID – Айдахо, прим. пер.):

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": {
    "bool": {
      "must": [
        { "match": { "age": "40" } }
      ],
      "must_not": [
        { "match": { "state": "ID" } }
      ]
    }
  }
}'
--------------------------------------------------

[[_executing_filters]]
=== Выполнение фильтрации

В предыдущем разделе, мы пропустили маленькую подробность, которая называется оценкой документа (поле `_score` в результатах поиска). Оценка - это числовое значение, которое является относительной мерой того, насколько документ совпадает с поисковым запросом, который мы создали. Большие значения оценки показывают более релевантные (более полно совпадающие) документы, меньшие значение - менее релевантные.

Но запросам не всегда нужно генерировать оценки, в особенности, когда они используются только для "фильтрации" списка документов. Elasticsearch определяет такие ситуации и автоматически оптимизирует выполнение запроса так, чтобы он не вычислял оценки.

<<query-dsl-bool-query, `bool` запрос>> который мы рассматривали в предыдущей секции, также поддерживает выражения `filter`, которые позволяю использовать запрос для того, чтобы ограничить количество документов, с которыми будет совпадение по другим предложениям, без изменения вычисления оценок. В качестве примера, давайте познакомимся с <<query-dsl-range-query,`range` запросом>>, который позволяет нам отфильтровать документы по диапазону значений. Обычно это фильтрация чисел или дат.

Данный пример использует `bool` запрос для возврата всех учётных записей, значение поля balance которых находится между 20000 и 30000, включительно. Другими словами, мы хотим найти учётные записи с балансом который больше или равен 20000 и меньше или равен 30000.

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "query": {
    "bool": {
      "must": { "match_all": {} },
      "filter": {
        "range": {
          "balance": {
            "gte": 20000,
            "lte": 30000
          }
        }
      }
    }
  }
}'
--------------------------------------------------

Как показанно выше, `bool` запрос содержит часть запроса `match_all` и часть запроса `range` (как `filter` часть). Вы можете подставить любые другие запросы в query и filter части. В примере выше, часть запроса `range` имеет смысл, потому что документы попадающие в дипазон range все являются 100% попаданием, т.е. нет одних документов, которые более релевантные, чем другие.

В дополнение к `match_all`, `match`, `bool` и `range` запросам, есть большое количество запросов других типов, но мы не будем говорить о них здесь. Поскольку мы уже имеем базовое понимание того, как это работает, это не должно быть слишком сложным для вас: применить эти знания в обучении и экспериментировании с другими типами запросов.

[[_executing_aggregations]]
=== Выполнение агрегаций

Агрегации предоставляют возможность группировки и извлечения статистистики из ваших данных. Наиболее лёгкий способ понять агрегации - это провести аналогию с SQL `GROUP BY` и агрегатными функциями SQL. В Elasticsearch у вас есть возможность выполнить поиск, возвращающий в одном и том же запросе совпавшие документы и в то же время возвращающий результаты агрегации, отдельно от совпавших документов. Это очень эффективно и мощно, в том смысле, что вы можете запускать запросы и множественные агрегации и получать результаты обоих (или одной) операций за один шаг, избегая лишней передачи данных по сети, используя краткий и простой API.

Для начала, данный пример группирует все учётные записи по штату проживания и затем возвращает десять первых штатов, отсортированные по количеству учётных записей в них в обратном порядке (также по умолчанию):

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state"
      }
    }
  }
}'
--------------------------------------------------

В SQL, вышеприведённая агрегация по смыслу походит на запрос:

[source,sh]
--------------------------------------------------
SELECT state, COUNT(*) FROM bank GROUP BY state ORDER BY COUNT(*) DESC
--------------------------------------------------

ответ (показан частично):

[source,sh]
--------------------------------------------------
  "hits" : {
    "total" : 1000,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "group_by_state" : {
      "buckets" : [ {
        "key" : "al",
        "doc_count" : 21
      }, {
        "key" : "tx",
        "doc_count" : 17
      }, {
        "key" : "id",
        "doc_count" : 15
      }, {
        "key" : "ma",
        "doc_count" : 15
      }, {
        "key" : "md",
        "doc_count" : 15
      }, {
        "key" : "pa",
        "doc_count" : 15
      }, {
        "key" : "dc",
        "doc_count" : 14
      }, {
        "key" : "me",
        "doc_count" : 14
      }, {
        "key" : "mo",
        "doc_count" : 14
      }, {
        "key" : "nd",
        "doc_count" : 14
      } ]
    }
  }
}
--------------------------------------------------

Вы можете увидеть, что здесь 21 учётная запись в AL (Алабаме), за которой следует 17 учётных записей в TX (Техас), за которой следует 15 учётных записей в ID (Айдахо) и так далее.

Обратите внимание, что мы установили `size=0`, чтобы не показывать совпавшие документы, потому что мы хотим видеть в ответе только результаты агрегации.

В отличии от предыдущей агрегации, данный пример вычисляет средний баланс учётной записи по штату (в отличие от первых 10 штатов, отсортированных по количеству учётных записей в обратном порядке):

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state"
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}'
--------------------------------------------------

Обратите внимание как мы вложили агрегацию `average_balance` внутрь агрегации `group_by_state`. Это общий образец для всех агрегаций. Вы можете произвольно вкладывать агрегации внутрь агрегаций для извлечения нужной степени получения итогов, которая нам требуются от наших данных.

В отличии от предыдущей агрегации, давайте теперь отсортируем средний баланс в обратном порядке:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "size": 0,
  "aggs": {
    "group_by_state": {
      "terms": {
        "field": "state",
        "order": {
          "average_balance": "desc"
        }
      },
      "aggs": {
        "average_balance": {
          "avg": {
            "field": "balance"
          }
        }
      }
    }
  }
}'
--------------------------------------------------

Данный пример демонстрирует как мы можем группировать возрастные ограничения с помощью квадратных скобкок (20-29, 30-29 и 40-49), затем по полу и наконец затем получить средний баланс учётной записи по кажому диапазону возраста, по полу:

[source,sh]
--------------------------------------------------
curl -XPOST 'localhost:9200/bank/_search?pretty' -d '
{
  "size": 0,
  "aggs": {
    "group_by_age": {
      "range": {
        "field": "age",
        "ranges": [
          {
            "from": 20,
            "to": 30
          },
          {
            "from": 30,
            "to": 40
          },
          {
            "from": 40,
            "to": 50
          }
        ]
      },
      "aggs": {
        "group_by_gender": {
          "terms": {
            "field": "gender"
          },
          "aggs": {
            "average_balance": {
              "avg": {
                "field": "balance"
              }
            }
          }
        }
      }
    }
  }
}'
--------------------------------------------------

Есть множество других возможностей агрегации, которые мы не будет подробно рассматривать здесь. <<search-aggregations, Справочное руководство по агрегациям>> является отличной точкой старта, если вы хотите больше экспериментов.

[[_conclusion]]
== Заключение

Elasticsearch - это одновременно простой и сложный продукт. До этого момента мы постигали азы, что это такое, как смотреть внутрь него, и как с ним работать, используя некоторые функции REST API. Я надеюсь, что этот урок дал вам лучшее понимание, что такое Elasticsearch и что более важно, вдохновило вас на дальнейшее эксперименты с остальными из его огромных возможностей!

